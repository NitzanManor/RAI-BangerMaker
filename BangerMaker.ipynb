{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://www.kaggle.com/code/karnikakapoor/lyrics-generator-rnn/notebook?select=Songs.csv",
   "id": "43bc73e228efc5c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Process",
   "id": "7e436d56383626de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T07:35:46.038841Z",
     "start_time": "2025-05-30T07:35:45.912009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"Songs.csv\")\n",
    "df = df.dropna(subset=[\"Lyrics\"])\n",
    "df = df[df[\"Lyrics\"].str.len() > 100]  # keep meaningful lyrics\n",
    "\n",
    "lyrics_ds = Dataset.from_pandas(df[[\"Lyrics\"]].rename(columns={\"Lyrics\": \"text\"}))"
   ],
   "id": "8a30c4367f30a6eb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:15:10.636166Z",
     "start_time": "2025-05-28T06:29:02.020160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return tokens\n",
    "\n",
    "tokenized_ds = lyrics_ds.map(tokenize_fn, batched=True)\n",
    "tokenized_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Load model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./banger_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=100,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "a1416fe9edec8aba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/744 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86ea569b23c24b088ab8f40303e336d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1116' max='1116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1116/1116 2:45:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.858100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.725100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.417800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.422900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.411600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.412700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.252900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.526100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.339200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.424100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.284600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1116, training_loss=2.531502740784785, metrics={'train_runtime': 9962.3499, 'train_samples_per_second': 0.224, 'train_steps_per_second': 0.112, 'total_flos': 291601907712000.0, 'train_loss': 2.531502740784785, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:28:48.320111Z",
     "start_time": "2025-05-28T09:28:13.549052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.save_model(\"./banger_gpt2_model\")\n",
    "tokenizer.save_pretrained(\"./banger_gpt2_model\")"
   ],
   "id": "19aa711c477ac208",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./banger_gpt2_model\\\\tokenizer_config.json',\n",
       " './banger_gpt2_model\\\\special_tokens_map.json',\n",
       " './banger_gpt2_model\\\\vocab.json',\n",
       " './banger_gpt2_model\\\\merges.txt',\n",
       " './banger_gpt2_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RUN THIS:",
   "id": "9242c66f25f36a0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T18:50:37.116448Z",
     "start_time": "2025-06-23T18:50:33.863756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import re\n",
    "\n",
    "def clean_lyrics(text):\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text).lower()\n",
    "\n",
    "df = pd.read_csv(\"Songs.csv\")\n",
    "df = df.dropna(subset=[\"Lyrics\"])\n",
    "df = df[df[\"Lyrics\"].str.len() > 100]  # keep meaningful lyrics\n",
    "  \n",
    "lyrics_ds = Dataset.from_pandas(df[[\"Lyrics\"]].rename(columns={\"Lyrics\": \"text\"}))\n",
    "\n",
    "dataset_texts = df[\"Lyrics\"].astype(str).map(clean_lyrics).tolist()\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./banger_gpt2_model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./banger_gpt2_model\")"
   ],
   "id": "fa1884b2bb198bba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T18:50:48.919497Z",
     "start_time": "2025-06-23T18:50:38.605132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def audited_generate(\n",
    "    prompt,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset_texts,\n",
    "    artists,\n",
    "    titles,\n",
    "    similarity_threshold=0.7,\n",
    "    max_length=150,\n",
    "    ngram_size=5,\n",
    "    max_semantic_sources=4\n",
    "):\n",
    "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def clean(text):\n",
    "        return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text).lower().strip()\n",
    "\n",
    "    def get_ngrams(text, n):\n",
    "        words = text.split()\n",
    "        return Counter([\" \".join(words[i:i + n]) for i in range(len(words) - n + 1)])\n",
    "\n",
    "    def normalize_lines(text):\n",
    "        return set(clean(line) for line in text.splitlines() if line.strip())\n",
    "\n",
    "    def get_opening_lines(text, num_lines=2):\n",
    "        lines = [line.strip().lower() for line in text.splitlines() if line.strip()]\n",
    "        return lines[:num_lines]\n",
    "\n",
    "    def get_most_repeated_lines(text):\n",
    "        lines = [line.strip().lower() for line in text.splitlines() if line.strip()]\n",
    "        return [line for line, count in Counter(lines).items() if count > 1]\n",
    "\n",
    "    def get_rhyme_endings(lines, length=3):\n",
    "        return [line[-length:] for line in lines if len(line) >= length]\n",
    "\n",
    "    def extract_named_entities(text):\n",
    "        doc = nlp(text)\n",
    "        return set(ent.text.lower() for ent in doc.ents)\n",
    "    \n",
    "    def lexical_overlap_score(text1, text2):\n",
    "        lines1 = normalize_lines(text1)\n",
    "        lines2 = normalize_lines(text2)\n",
    "        return len(lines1 & lines2)\n",
    "    \n",
    "    # --- Prepare Dataset ---\n",
    "    cleaned_dataset = [clean(txt) for txt in dataset_texts]\n",
    "    dataset_embeddings = embed_model.encode(cleaned_dataset, convert_to_tensor=True)\n",
    "\n",
    "    # --- Generate Lyrics ---\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    gen_output = generator(prompt, max_length=max_length, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)[0]['generated_text']\n",
    "    cleaned_gen = clean(gen_output)\n",
    "    gen_embedding = embed_model.encode(cleaned_gen, convert_to_tensor=True)\n",
    "\n",
    "    # === Audit ===\n",
    "    risk_score = 0.0\n",
    "    reasons = []\n",
    "\n",
    "    # Semantic similarity\n",
    "    similarities = util.cos_sim(gen_embedding, dataset_embeddings)[0]\n",
    "    strong_similarities = [float(sim) for sim in similarities if sim >= similarity_threshold]\n",
    "    top_score = float(similarities.max())\n",
    "    most_similar_idx = int(similarities.argmax())\n",
    "    song_sem = titles[most_similar_idx]\n",
    "    artist_sem = artists[most_similar_idx]\n",
    "    \n",
    "    print(f\"Top semantic similarity: {top_score:.3f}\\n\")\n",
    "    print(f\"Closest match (semantic): \\\"{song_sem}\\\" written by {artist_sem}\")\n",
    "    if 1 <= len(strong_similarities) <= max_semantic_sources:\n",
    "        risk_score += 0.8\n",
    "        reasons.append(\"High semantic similarity with known song\")\n",
    "    \n",
    "    lexical_scores = [lexical_overlap_score(gen_output, ds) for ds in dataset_texts]\n",
    "    most_lexical_idx = int(max(range(len(lexical_scores)), key=lambda i: lexical_scores[i]))\n",
    "    lexical_overlap = lexical_scores[most_lexical_idx]\n",
    "    song_lex = titles[most_lexical_idx]\n",
    "    artist_lex = artists[most_lexical_idx]\n",
    "    \n",
    "    print(f\"Closest match (line overlap): \\\"{song_lex}\\\" written by {artist_lex}\")\n",
    "    print(f\"Line overlap count: {lexical_overlap}\\n\")\n",
    "    if lexical_overlap > 0:\n",
    "        reasons.append(f'Similar to \"{song_lex}\" by {artist_lex}')\n",
    "    \n",
    "    # N-gram check\n",
    "    gen_ngrams = get_ngrams(cleaned_gen, ngram_size)\n",
    "    for song in cleaned_dataset:\n",
    "        song_ngrams = get_ngrams(song, ngram_size)\n",
    "        if sum((gen_ngrams & song_ngrams).values()) > 0:\n",
    "            risk_score += 1.0\n",
    "            reasons.append(f\"{ngram_size}-word exact phrase match\")\n",
    "            break\n",
    "\n",
    "    # Line-level match\n",
    "    gen_lines = normalize_lines(gen_output)\n",
    "    matched_lines = []\n",
    "    for song in dataset_texts:\n",
    "        dataset_lines = normalize_lines(song)\n",
    "        matches = dataset_lines & gen_lines\n",
    "        if matches:\n",
    "            matched_lines.extend(matches)\n",
    "    \n",
    "    if matched_lines:\n",
    "        risk_score += 1.0\n",
    "        reasons.append(f\"{len(matched_lines)} line-level matches found\")\n",
    "                \n",
    "    # Unique line detection\n",
    "    for line in gen_lines:\n",
    "        occurrence_count = sum(1 for song in cleaned_dataset if line in song)\n",
    "        if occurrence_count == 1:\n",
    "            risk_score += 1.0\n",
    "            reasons.append(f\"Unique line match: \\\"{line}\\\" appears only once in dataset\")\n",
    "            break\n",
    "\n",
    "    # Structural/style checks\n",
    "    gen_opening = get_opening_lines(gen_output)\n",
    "    gen_repeats = set(get_most_repeated_lines(gen_output))\n",
    "    gen_rhymes = get_rhyme_endings(list(gen_lines))\n",
    "\n",
    "    song_ref = dataset_texts[most_similar_idx]\n",
    "    dataset_opening = get_opening_lines(song_ref)\n",
    "    dataset_repeats = set(get_most_repeated_lines(song_ref))\n",
    "    dataset_rhymes = get_rhyme_endings(song_ref.splitlines())\n",
    "\n",
    "    if any(line in dataset_opening for line in gen_opening):\n",
    "        risk_score += 0.5\n",
    "        reasons.append(\"Opening line matches known song\")\n",
    "\n",
    "    if gen_repeats & dataset_repeats:\n",
    "        risk_score += 0.4\n",
    "        reasons.append(\"Repeated line pattern (chorus-style)\")\n",
    "\n",
    "    if len(gen_rhymes) >= 3 and len(set(gen_rhymes) & set(dataset_rhymes)) >= 3:\n",
    "        risk_score += 0.4\n",
    "        reasons.append(\"Similar rhyme endings\")\n",
    "\n",
    "    # Named entities\n",
    "    gen_ents = extract_named_entities(gen_output)\n",
    "    for song in dataset_texts:\n",
    "        dataset_ents = extract_named_entities(song)\n",
    "        if gen_ents & dataset_ents:\n",
    "            risk_score += 0.3\n",
    "            reasons.append(\"Shared named entities (e.g., people, brands)\")\n",
    "            break\n",
    "\n",
    "    # Toxic content filter\n",
    "    if any(word in cleaned_gen for word in [\"kill\", \"rape\", \"bitch\", \"n*\", \"f*\", \"drugs\", \"cocaine\"]):\n",
    "        print(\"Rejected: toxic language detected.\\n\")\n",
    "        print(\"Reasons:\", reasons)\n",
    "        print(\"Generated Song:\\n\", gen_output)\n",
    "    else:\n",
    "        # Final decision\n",
    "        print(\"Generated Song:\\n\", gen_output)\n",
    "        print(\"\\nReasons:\", reasons)\n",
    "        print(\"\\nRisk Score:\", round(min(1.0, risk_score), 2))\n",
    "        \n",
    "        if risk_score >= 0.8:\n",
    "            print(\"\\nRejected due to high similarity.\\n\")\n",
    "            print(\"Legal Warning: You may be exposed to legal action from the original song's writer!\\n\")\n",
    "        else:\n",
    "            print(\"Passed audit\")"
   ],
   "id": "6eba7034476b9bb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T09:33:37.210826Z",
     "start_time": "2025-06-14T09:31:18.073961Z"
    }
   },
   "cell_type": "code",
   "source": "audited_generate(\"Vintage tee, brand new phone\", model, tokenizer, dataset_texts, df[\"Artist\"].tolist(), df[\"Title\"].tolist())",
   "id": "9e5afcaa45ec98f5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top semantic similarity: 0.527\n",
      "\n",
      "Closest match (semantic): \"Born to Die\" by Lana Del Rey\n",
      "Closest match (line overlap): \"cardigan\" by Taylor Swift\n",
      "Line overlap count: 1\n",
      "Generated Song:\n",
      " Vintage tee, brand new phone\n",
      "I was in the band\n",
      "I walked to the door\n",
      "To be alone\n",
      "I tried to get out of sight\n",
      "I cried 'til I couldn't sleep\n",
      "But I got out\n",
      "In that one song, we all sing\n",
      "A song I'm really proud of\n",
      "When you sing it on the radio, then everybody just loves\n",
      "And I just think they know\n",
      "All these folks in the room don't follow\n",
      "How we got here\n",
      "I think I'm gonna try\n",
      "Oh, I will try\n",
      "Oh, no, just try\n",
      "Oh, I will try\n",
      "\n",
      "Wondering about this song, but\n",
      "What am I gonna do?\n",
      "I can't tell to you that I'm\n",
      "Reasons: ['Similar to \"cardigan\" by Taylor Swift', '5-word exact phrase match', '1 line-level matches found', 'Unique line match: \"vintage tee brand new phone\" appears only once in dataset', 'Shared named entities (e.g., people, brands)']\n",
      "Risk Score: 1.0\n",
      "Rejected due to high similarity.\n",
      "\n",
      "Legal Warning: You may be exposed to legal action from the original song's artist.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1ff56bc9549fbe63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
