{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://www.kaggle.com/code/karnikakapoor/lyrics-generator-rnn/notebook?select=Songs.csv",
   "id": "43bc73e228efc5c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Process",
   "id": "7e436d56383626de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T07:35:46.038841Z",
     "start_time": "2025-05-30T07:35:45.912009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# \n",
    "# df = pd.read_csv(\"Songs.csv\")\n",
    "# df = df.dropna(subset=[\"Lyrics\"])\n",
    "# df = df[df[\"Lyrics\"].str.len() > 100]  # keep meaningful lyrics\n",
    "#   \n",
    "# lyrics_ds = Dataset.from_pandas(df[[\"Lyrics\"]].rename(columns={\"Lyrics\": \"text\"}))"
   ],
   "id": "8a30c4367f30a6eb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:15:10.636166Z",
     "start_time": "2025-05-28T06:29:02.020160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "# \n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# \n",
    "# def tokenize_fn(example):\n",
    "#     tokens = tokenizer(\n",
    "#         example[\"text\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=256,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "#     return tokens\n",
    "# \n",
    "# tokenized_ds = lyrics_ds.map(tokenize_fn, batched=True)\n",
    "# tokenized_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "# \n",
    "# # Load model\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# \n",
    "# # Define training args\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./banger_model\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     save_steps=100,\n",
    "#     logging_steps=20,\n",
    "#     save_total_limit=2,\n",
    "#     prediction_loss_only=True\n",
    "# )\n",
    "# \n",
    "# # Train\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_ds,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "# \n",
    "# trainer.train()"
   ],
   "id": "a1416fe9edec8aba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/744 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86ea569b23c24b088ab8f40303e336d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1116' max='1116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1116/1116 2:45:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.858100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.725100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.417800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.422900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.411600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.412700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.252900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.526100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.339200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.424100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.284600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1116, training_loss=2.531502740784785, metrics={'train_runtime': 9962.3499, 'train_samples_per_second': 0.224, 'train_steps_per_second': 0.112, 'total_flos': 291601907712000.0, 'train_loss': 2.531502740784785, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:28:48.320111Z",
     "start_time": "2025-05-28T09:28:13.549052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trainer.save_model(\"./banger_gpt2_model\")\n",
    "# tokenizer.save_pretrained(\"./banger_gpt2_model\")"
   ],
   "id": "19aa711c477ac208",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./banger_gpt2_model\\\\tokenizer_config.json',\n",
       " './banger_gpt2_model\\\\special_tokens_map.json',\n",
       " './banger_gpt2_model\\\\vocab.json',\n",
       " './banger_gpt2_model\\\\merges.txt',\n",
       " './banger_gpt2_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RUN THIS:",
   "id": "9242c66f25f36a0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T07:34:50.924995Z",
     "start_time": "2025-05-30T07:34:39.000521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def audited_generate(\n",
    "    prompt,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset_texts,\n",
    "    similarity_threshold=0.85,\n",
    "    max_length=150,\n",
    "    ngram_size=5\n",
    "):\n",
    "    # === Setup ===\n",
    "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def clean(text):\n",
    "        return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text).lower()\n",
    "\n",
    "    cleaned_dataset = [clean(txt) for txt in dataset_texts]\n",
    "    dataset_embeddings = embed_model.encode(cleaned_dataset, convert_to_tensor=True)\n",
    "\n",
    "    # === Generate text ===\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    gen_output = generator(prompt, max_length=max_length, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)[0]['generated_text']\n",
    "    cleaned_gen = clean(gen_output)\n",
    "    gen_embedding = embed_model.encode(cleaned_gen, convert_to_tensor=True)\n",
    "\n",
    "    # === Similarity check ===\n",
    "    similarities = util.cos_sim(gen_embedding, dataset_embeddings)[0]\n",
    "    top_score = float(similarities.max())\n",
    "    most_similar_idx = int(similarities.argmax())\n",
    "\n",
    "    print(f\"\\nSimilarity to dataset: {top_score:.3f}\")\n",
    "    print(f\"\\nClosest known lyric (manual inspection):\\n{dataset_texts[most_similar_idx]}...\\n\")\n",
    "\n",
    "    if top_score >= similarity_threshold:\n",
    "        print(\"Rejected: too similar to existing lyric (semantic similarity).\")\n",
    "        return None\n",
    "\n",
    "    # === Line-level plagiarism check ===\n",
    "    gen_lines = set([line.strip() for line in gen_output.lower().splitlines() if line.strip()])\n",
    "    for line in dataset_texts[most_similar_idx].lower().splitlines():\n",
    "        if line.strip() in gen_lines and line.strip() != \"\":\n",
    "            print(f\"Rejected: line-level match found — \\\"{line.strip()}\\\"\")\n",
    "            return None\n",
    "\n",
    "    # === N-gram overlap check ===\n",
    "    def get_ngrams(text, n):\n",
    "        words = text.split()\n",
    "        return Counter([\" \".join(words[i:i+n]) for i in range(len(words)-n+1)])\n",
    "\n",
    "    gen_ngrams = get_ngrams(cleaned_gen, ngram_size)\n",
    "    song_ngrams = get_ngrams(cleaned_dataset[most_similar_idx], ngram_size)\n",
    "    overlap = gen_ngrams & song_ngrams\n",
    "    if sum(overlap.values()) > 0:\n",
    "        print(f\"Rejected: n-gram overlap detected ({sum(overlap.values())} matches)\")\n",
    "        print(\"Common phrase example:\", list(overlap.keys())[0])\n",
    "        return None\n",
    "\n",
    "    # === Toxicity filter ===\n",
    "    if any(word in cleaned_gen for word in [\"kill\", \"rape\", \"bitch\", \"n***\", \"f***\"]):\n",
    "        print(\"Rejected: toxic language detected.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Passed audit.\\n\")\n",
    "    return gen_output"
   ],
   "id": "d51ea49926895672",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T07:33:52.888028Z",
     "start_time": "2025-05-30T07:33:40.982279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def clean_lyrics(text):\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text).lower()\n",
    "\n",
    "df = pd.read_csv(\"Songs.csv\")\n",
    "df = df.dropna(subset=[\"Lyrics\"])\n",
    "df = df[df[\"Lyrics\"].str.len() > 100]  # keep meaningful lyrics\n",
    "  \n",
    "lyrics_ds = Dataset.from_pandas(df[[\"Lyrics\"]].rename(columns={\"Lyrics\": \"text\"}))\n",
    "\n",
    "dataset_texts = df[\"Lyrics\"].astype(str).map(clean_lyrics).tolist()\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./banger_gpt2_model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./banger_gpt2_model\")"
   ],
   "id": "fa1884b2bb198bba",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity to dataset: 0.101\n",
      "\n",
      "Passed audit.\n",
      "\n",
      "Under the moonlight she painted my face\n",
      "With your hand on my cheek\n",
      "And I kissed her over the face of love\n",
      "In exchange of an oath to you\n",
      "Bam, I love you\n",
      "You're my best friend tonight\n",
      "\n",
      "Come in and ride home tonight\n",
      "In this quiet, bright city\n",
      "I'm riding on your shoulders\n",
      "If you wanna share you should do the work for me\n",
      "Let the moon out for the night\n",
      "The sky is filled with sunshine\n",
      "Oh, come\n"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "output = audited_generate(\"Under the moonlight she\", model, tokenizer, dataset_texts)\n",
    "if output:\n",
    "    print(output)"
   ],
   "id": "d0e44386865e27ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity to dataset: 0.546\n",
      "\n",
      "Closest known lyric (manual inspection):\n",
      "perhaps love is like a resting place a shelter from the storm\n",
      "it exists to give you comfort it is there to keep you warm\n",
      "and in those times of trouble when you are most alone\n",
      "the memory of love will bring you home\n",
      "perhaps love is like a window perhaps an open door\n",
      "it invites you to come closer it wa...\n",
      "\n",
      "✅ Passed audit.\n",
      "\n",
      "My love, love, love, love\n",
      "Like a knife through the heart of you\n",
      "As I've held to you and I've broken you down\n",
      "\n",
      "What do you expect from love, love, love?\n",
      "So far I've been the one not to love you\n",
      "And what do you expect from a lover?\n",
      "\n",
      "(But don't take this at face value, can you?)\n",
      "Well we'll still be with you for a little while\n",
      "Maybe just in a little bit of\n"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "output = audited_generate(\"My love\", model, tokenizer, dataset_texts)\n",
    "\n",
    "if output:\n",
    "    print(output)"
   ],
   "id": "7bc8f50874474c11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity to dataset: 0.489\n",
      "\n",
      "Closest known lyric (manual inspection):\n",
      "stay for a little while\n",
      "share a glass beside the fire\n",
      "time goes by so fast so fast\n",
      "lets make it last\n",
      "\n",
      "stay stay and talk awhile\n",
      "share a laugh share a smile\n",
      "days go by so fast so fast\n",
      "lets make it last\n",
      "\n",
      "oh years they roll and fall away\n",
      "all the colors slowly fade to grey\n",
      "\n",
      "stay or watch another round\n",
      "s...\n",
      "\n",
      "✅ Passed audit.\n",
      "\n",
      "Watermelon and a soft brown rose\n",
      "Cinnamon rolled for the day\n",
      "Cinnamon pie with caramelise frosting\n",
      "\n",
      "The light's on the porch, summer's early\n",
      "Beware the winter's coming\n",
      "The lights, oh, are bright\n",
      "\n",
      "I don't feel so bright anymore\n",
      "The sun's out of the window, summer's hot\n",
      "\n",
      "Come and stay with me, come\n",
      "\n",
      "Come and stay with me, come\n",
      "\n",
      "I'm so scared of the world now,\n"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "output = audited_generate(\"Watermelon\", model, tokenizer, dataset_texts)\n",
    "\n",
    "if output:\n",
    "    print(output)"
   ],
   "id": "bec74b6bda68649d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity to dataset: 0.565\n",
      "\n",
      "Closest known lyric (manual inspection):\n",
      "i am not trying to seduce you\n",
      "would you like me to seduce you\n",
      "is that what youre trying to tell me\n",
      "yall ready\n",
      "\n",
      "hey youre just too funky for me\n",
      "i gotta get inside of you\n",
      "and ill show you heaven\n",
      "if you let me\n",
      "hey youre just too funky for me\n",
      "i gotta get inside i gotta get inside\n",
      "i gotta get inside of you\n",
      "so when will that be\n",
      "\n",
      "i watch your fingers working overtime overtime\n",
      "ive got to thinking that they should be mine oh\n",
      "id love to see you naked baby\n",
      "id like to think that sometime maybe tonight\n",
      "if thats alright yeah\n",
      "\n",
      "hey youre just too funky for me\n",
      "i gotta get inside of you\n",
      "wont let you go i wont let you no no\n",
      "hey youre just too funky for me\n",
      "i gotta get inside i gotta get inside\n",
      "i gotta get inside of you\n",
      "ill make you love me\n",
      "\n",
      "i watch you drinking and i take my time take my time\n",
      "i watch you sinking all of that cheap red wine oh\n",
      "ive got to see you naked baby\n",
      "id like to think that sometime maybe tonight\n",
      "my goals in sight yeah\n",
      "\n",
      "baby baby baby why do you do this to me\n",
      "wont let you go wont let you go\n",
      "youre such a youre such a\n",
      "baby baby baby why do you do this to me\n",
      "i got to know i got to know\n",
      "im gonna be the kind of lover that you never had\n",
      "hey youre just too funky\n",
      "youre never gonna have another lover in your bed\n",
      "youre just too funky for me\n",
      "\n",
      "would you like me to seduce you\n",
      "is that what youre trying to tell me\n",
      "\n",
      "everybody wants a lover like that baby\n",
      "everybody wants a lover like that\n",
      "yeah yeah\n",
      "everybody wants a lover like that\n",
      "everybody everybody\n",
      "everybody wants a lover like that\n",
      "is that what youre trying to tell me\n",
      "everybody wants a lover\n",
      "everybody wants a lover like that\n",
      "everybody wants a lover like that\n",
      "everybody wants a lover like that\n",
      "everybody wants a lover like that\n",
      "\n",
      "would you like me to seduce you\n",
      "youre such a youre such a\n",
      "would you like me to seduce you\n",
      "yeah yeah\n",
      "would you like me to seduce you\n",
      "youre such a youre such a\n",
      "yeah yeah\n",
      "\n",
      "would you stop playing with that radio of yours\n",
      "im trying to get to sleep...\n",
      "\n",
      "Passed audit.\n",
      "\n",
      "I just wanna party on at the opera\n",
      "A-Sing my way around, on my own\n",
      "I wanna dance in the crowd\n",
      "I wanna dance in the night\n",
      "When my lady comes to you\n",
      "You'll know me when I show up\n",
      "\n",
      "It ain't been a long time\n",
      "It's just a long time to come\n",
      "I wanna party in the evening\n",
      "I wanna dance in the crowd\n",
      "When my lady comes there\n",
      "I got a drink by the bar\n",
      "Wanna dance\n",
      "I wanna party up all night\n",
      "It ain't been a long time\n",
      "It's just a long time, ain't it?\n",
      "\n",
      "You can't stand my man\n",
      "Sometimes you gotta stop\n",
      "I want to try to stop you\n",
      "It ain\n"
     ]
    }
   ],
   "execution_count": 18,
   "source": [
    "output = audited_generate(\"I just wanna party\", model, tokenizer, dataset_texts)\n",
    "\n",
    "if output:\n",
    "    print(output)"
   ],
   "id": "ac173d7f7d215bd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T07:36:53.721437Z",
     "start_time": "2025-05-30T07:35:49.457986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = audited_generate(\"Hello\", model, tokenizer, dataset_texts)\n",
    "\n",
    "if output:\n",
    "    print(output)"
   ],
   "id": "c19b866c33e93d95",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity to dataset: 0.528\n",
      "\n",
      "Closest known lyric (manual inspection):\n",
      "billie\n",
      "\n",
      "what do you want from me why dont you run from me\n",
      "what are you wondering what do you know\n",
      "why arent you scared of me why do you care for me\n",
      "when we all fall asleep where do we go\n",
      "\n",
      "come here\n",
      "say it spit it out what is it exactly\n",
      "youre payin is the amount cleanin you out am i satisfactory\n",
      "today im thinkin about the things that are deadly\n",
      "the way im drinkin you down\n",
      "like i wanna drown like i wanna end me\n",
      "\n",
      "step on the glass staple your tongue ahh\n",
      "bury a friend try to wake up ahahh\n",
      "cannibal class killing the son ahh\n",
      "bury a friend i wanna end me\n",
      "\n",
      "i wanna end me\n",
      "i wanna i wanna i wanna end me\n",
      "i wanna i wanna i wanna\n",
      "\n",
      "what do you want from me why dont you run from me\n",
      "what are you wondering what do you know\n",
      "why arent you scared of me why do you care for me\n",
      "when we all fall asleep where do we go\n",
      "\n",
      "listen\n",
      "keep you in the dark what had you expected\n",
      "me to make you my art and make you a star\n",
      "and get you connected\n",
      "ill meet you in the park ill be calm and collected\n",
      "but we knew right from the start that youd fall apart\n",
      "cause im too expensive\n",
      "its probably somethin that shouldnt be said out loud\n",
      "honestly i thought that i would be dead by now wow\n",
      "calling security keepin my head held down\n",
      "bury the hatchet or bury a friend right now\n",
      "\n",
      "the debt i owe gotta sell my soul\n",
      "cause i cant say no no i cant say no\n",
      "then my limbs all froze and my eyes wont close\n",
      "and i cant say no i cant say no\n",
      "careful\n",
      "\n",
      "step on the glass staple your tongue ahh\n",
      "bury a friend try to wake up ahahh\n",
      "cannibal class killing the son ahh\n",
      "bury a friend i wanna end me\n",
      "\n",
      "i wanna end me\n",
      "i wanna i wanna i wanna end me\n",
      "i wanna i wanna i wanna\n",
      "\n",
      "what do you want from me why dont you run from me\n",
      "what are you wondering what do you know\n",
      "why arent you scared of me why do you care for me\n",
      "when we all fall asleep where do we go...\n",
      "\n",
      "Passed audit.\n",
      "\n",
      "Hello, are you home from work tonight?\n",
      "Yeah, I came up from out of nowhere\n",
      "Took a few deep breaths, and then stood to watch the sunrise\n",
      "\n",
      "You're home from work tonight\n",
      "I walked out to my house on Park Row\n",
      "Well, I gotta do something\n",
      "Something good to do tonight\n",
      "I'm on my way down\n",
      "I don't wanna run, so why did you go running home?\n",
      "I don't wanna go running when I'm walking down on Broadway\n",
      "Well, I gotta do something good to do tonight\n",
      "I'm on my way down\n",
      "I don't wanna run, so why did you go running home?\n",
      "Hmmm, I kinda want to go home tonight\n",
      "I wanna go home\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5eb66713fbfd14cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
